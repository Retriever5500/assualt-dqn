{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c076162f",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# imports from memory.py\n",
    "import numpy as np\n",
    "\n",
    "# imports from wrappers.py\n",
    "import gymnasium as gym\n",
    "from PIL import Image\n",
    "from ale_py import ALEInterface\n",
    "\n",
    "# imports from agent.py \n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "# imports from train.py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Misc.\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597e4e9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d800b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNet(nn.Module):\n",
    "    def __init__(self, num_of_actions):\n",
    "        super(DQNNet, self).__init__()\n",
    "        self.num_of_actions = num_of_actions\n",
    "\n",
    "        # images should be preprocessed (extract luminance channel from RGB channels) by Ï† defined in the paper.\n",
    "\n",
    "        # in: (4, 84, 84) - out: (32, 20, 20)\n",
    "        # reLU should be applied on the outputs\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size= (8, 8), stride=4) \n",
    "\n",
    "        # in: (32, 20, 20) - out: (64, 9, 9)\n",
    "        # reLU should be applied on the outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size= (4, 4), stride=2)\n",
    "\n",
    "        # in: (64, 9, 9) - out: (64, 7, 7)\n",
    "        # reLU should be applied on the outputs\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size= (3, 3), stride=1)\n",
    "\n",
    "        # flattening should be applied here before feeding into fc1\n",
    "\n",
    "        # in: 64 * 7 * 7 = (3136, ) - out: (512, ) \n",
    "        # reLU should be applied on the outputs\n",
    "        self.fc1 = nn.Linear(in_features=3136, out_features=512)\n",
    "\n",
    "        # in: (512, ) - out: (7, )\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=num_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "         \n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "         \n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        x = nn.Flatten(start_dim=1, end_dim=-1)(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc74820",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d54fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_num_transitions, mini_batch_size, device=\"cpu\"):\n",
    "        self.lst = []\n",
    "        self.max_num_transitions = max_num_transitions\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.device = device\n",
    "\n",
    "    # this function returns the number of stored transitions\n",
    "    def __len__(self):\n",
    "        return len(self.lst)\n",
    "    \n",
    "    def append(self, curr_state, action, reward, next_state, done):\n",
    "        if len(self.lst) == self.max_num_transitions:\n",
    "            self.lst.pop(0)\n",
    "\n",
    "        trans = (curr_state, action, reward, next_state, done)\n",
    "        self.lst.append(trans)\n",
    "    \n",
    "    def sample_mini_batch(self):\n",
    "        if len(self.lst) < self.mini_batch_size:\n",
    "            raise Exception('Don\\'t try to sample mini-batches while number of stored transitions < mini_batch_size')\n",
    "        \n",
    "        idxs = np.random.randint(0, len(self.lst), self.mini_batch_size)\n",
    "        samples = [self.lst[idx] for idx in idxs]\n",
    "\n",
    "        zipped_content = tuple(zip(*samples))\n",
    "        dtypes = [torch.float32, torch.int, torch.bool, torch.float32, torch.float32]\n",
    "\n",
    "        mini_batch = [\n",
    "            torch.tensor(np.array(zipped_content[i]), dtype=dtypes[i]).to(self.device)\n",
    "            for i in range(len(zipped_content))\n",
    "        ]\n",
    "        \n",
    "        return mini_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6378f",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac47c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariImage(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Gym wrapper to preprocess the environments observations (frames)\n",
    "    The wrapper applies frameskip and stacks frames together\n",
    "    The same action is taken in each frame of a stack\n",
    "\n",
    "    :param env: Environment to wrap\n",
    "    :param image_shape: The output shape of the image\n",
    "    :param frame_skip: The amount of frames that stack, also the same action is applied\n",
    "    \"\"\"\n",
    "    def __init__(self, env, image_shape=(84, 84), frame_skip=4):\n",
    "        super().__init__(env)\n",
    "        self.image_shape = image_shape\n",
    "        self.frame_skip = frame_skip\n",
    "\n",
    "        obs_shape = (frame_skip, self.image_shape[0], self.image_shape[1])\n",
    "        self.observation_space = gym.spaces.Box(shape=obs_shape, low=0, high=1, dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        observations = []\n",
    "\n",
    "        obs, info = self.env.reset()\n",
    "        obs = self._process_observations(obs)\n",
    "        observations.append(obs)\n",
    "\n",
    "        for i in range(self.frame_skip - 1):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(0) # Do nothing\n",
    "            obs = self._process_observations(obs)\n",
    "            observations.append(obs)\n",
    "\n",
    "        observation = np.stack(observations)\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        observations = []\n",
    "        total_reward = 0\n",
    "        for i in range(self.frame_skip):\n",
    "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            obs = self._process_observations(obs)\n",
    "            observations.append(obs)\n",
    "            total_reward += reward\n",
    "\n",
    "        observation = np.stack(observations)\n",
    "\n",
    "        return observation, total_reward, terminated, truncated, info\n",
    "\n",
    "    def _process_observations(self, obs):\n",
    "        image = Image.fromarray(obs)\n",
    "        image = image.convert('L')\n",
    "        image = image.resize((self.image_shape[1], self.image_shape[0]))\n",
    "        image_array = np.array(image).astype(np.float32)\n",
    "        image_array /= 255\n",
    "        return image_array\n",
    "\n",
    "\n",
    "class ClipReward(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Gym wrapper to clip rewards\n",
    "\n",
    "    :param env: Environment to wrap\n",
    "    :param min_reward: The minimum reward\n",
    "    :param max_reward: The maximum reward\n",
    "    \"\"\"\n",
    "    def __init__(self, env, min_reward=-1, max_reward=1):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        reward = float(np.clip(reward, self.min_reward, self.max_reward))\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3877e2",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bfa8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_of_actions=7, network=None, lr=0.00025, gamma=0.99, eps=1.0,\n",
    "        eps_fframe=1e6, eps_final=0.1, minibatch_size=32, min_training_step=1000,\n",
    "        max_num_transitions=50000, target_interval=10000, device=\"cpu\"):   \n",
    "\n",
    "        self.num_of_actions = num_of_actions\n",
    "        if(network == None):\n",
    "            network = DQNNet(num_of_actions)\n",
    "        self.network = network.to(device)\n",
    "        self.target_network = copy.deepcopy(network)\n",
    "        self.target_interval = target_interval\n",
    "        self.learn_count = 0\n",
    "        # Hyperparameters taken from the paper\n",
    "        self.optim = torch.optim.RMSprop(network.parameters(), lr=lr, alpha=0.95, eps=0.01, momentum=0.95)\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "        self.eps = eps\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_step = (eps - eps_final) / eps_fframe\n",
    "        self.gamma = gamma\n",
    "        self.min_training_step = min_training_step\n",
    "\n",
    "        self.memory = Memory(max_num_transitions=max_num_transitions, mini_batch_size=32, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.network.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        torch.save(self.network.state_dict(), model_path)\n",
    "\n",
    "    def store_transition(self, obs, action, reward, done, next_obs):\n",
    "        self.memory.append(obs, action, reward, done, next_obs)\n",
    "\n",
    "    def choose_action(self, obs, eps=None):\n",
    "        if(eps == None):\n",
    "            eps = self.eps\n",
    "\n",
    "        if(random.random() < eps):\n",
    "            return random.randint(0, self.num_of_actions - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action_values = self.network(obs)\n",
    "                return torch.argmax(action_values).item()\n",
    "\n",
    "    def learn(self):\n",
    "        if(len(self.memory) < self.min_training_step):\n",
    "            return\n",
    "\n",
    "        obss, actions, rewards, dones, next_obss = self.memory.sample_mini_batch()\n",
    "\n",
    "        ys = rewards + 0.0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            next_qvals = self.target_network(next_obss)\n",
    "            ys[dones == 0] += self.gamma * torch.max(next_qvals, dim=1)[0][dones == 0]\n",
    "        \n",
    "        qvals = self.network(obss)\n",
    "        ys_p = qvals[torch.arange(qvals.size(0), device=self.device), actions]\n",
    "\n",
    "        loss = F.mse_loss(ys, ys_p)\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        self.eps = max(self.eps - self.eps_step, self.eps_final)\n",
    "\n",
    "        self.learn_count += 1\n",
    "\n",
    "        if(self.learn_count % self.target_interval == 0):\n",
    "            self.target_network = copy.deepcopy(self.network)\n",
    "            print(\"Updated target network\")\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd5421",
   "metadata": {},
   "source": [
    "# Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8e37e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logs(game_id, total_interactions, episode_cnt, history_of_total_losses, history_of_total_rewards):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    x = np.arange(1, episode_cnt + 1)\n",
    "    sns.lineplot(x=x, y=history_of_total_losses, ax=axs[0])\n",
    "    axs[0].set_title('Total Loss over Different Episodes')\n",
    "    axs[0].set_xlabel('Episodes')\n",
    "    axs[0].set_ylabel('Total MSE Loss')\n",
    "    # axs[0].legend()\n",
    "\n",
    "    sns.lineplot(x=x, y=history_of_total_rewards, ax=axs[1])\n",
    "    axs[1].set_title('Total Rewards over each Episodes')\n",
    "    axs[1].set_xlabel('Episodes')\n",
    "    axs[1].set_ylabel('Total Reward')\n",
    "    # axs[1].legend()\n",
    "\n",
    "    plt.suptitle('Total Loss & Reward over each Episodes \\n ' )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32541668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Environment for the Game ALE/Breakout-v5 has been Initialized.\n",
      "Starting the Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m episode_finished:\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# chosing action - observing the outcome - storing in replay buffer - learning \u001b[39;00m\n\u001b[32m     43\u001b[39m     action = agent.choose_action(obs.unsqueeze(\u001b[32m0\u001b[39m).to(device))\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     next_obs, reward, terminated, truncated, info = \u001b[43mwrapped_env\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     next_obs, action = torch.tensor(next_obs), torch.tensor(action)\n\u001b[32m     47\u001b[39m     agent.store_transition(obs, action, reward, terminated \u001b[38;5;129;01mor\u001b[39;00m truncated, next_obs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mAtariImage.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     37\u001b[39m total_reward = \u001b[32m0\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.frame_skip):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     obs = \u001b[38;5;28mself\u001b[39m._process_observations(obs)\n\u001b[32m     41\u001b[39m     observations.append(obs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mClipReward.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m     reward = \u001b[38;5;28mfloat\u001b[39m(np.clip(reward, \u001b[38;5;28mself\u001b[39m.min_reward, \u001b[38;5;28mself\u001b[39m.max_reward))\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obs, reward, terminated, truncated, info\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_for_ai/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[39m, in \u001b[36mOrderEnforcing.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_reset:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[33m\"\u001b[39m\u001b[33mCannot call env.step() before calling env.reset()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_for_ai/lib/python3.12/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_for_ai/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[39m, in \u001b[36mPassiveEnvChecker.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m.env, action)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_for_ai/lib/python3.12/site-packages/ale_py/env.py:305\u001b[39m, in \u001b[36mAtariEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    303\u001b[39m reward = \u001b[32m0.0\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     reward += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43male\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m is_terminal = \u001b[38;5;28mself\u001b[39m.ale.game_over(with_truncation=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    308\u001b[39m is_truncated = \u001b[38;5;28mself\u001b[39m.ale.game_truncated()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# cofiguration of the environment\n",
    "game_id = 'ALE/Breakout-v5'\n",
    "max_total_interactions = 5000000\n",
    "frame_skip = 4\n",
    "env = gym.make(id=game_id, **{'frameskip':1})\n",
    "clip_reward_wrapper = ClipReward(env)\n",
    "atari_image_wrapper = AtariImage(clip_reward_wrapper)\n",
    "# add other wrappers if needed\n",
    "# ...\n",
    "wrapped_env = atari_image_wrapper # set to the last applied wrapper for more convinent naming \n",
    "\n",
    "print(f'The Environment for the Game {game_id} has been Initialized.')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# configuration of the agent\n",
    "agent = Agent( num_of_actions=4, device=device) # we keep the arguments as default\n",
    "\n",
    "\n",
    "# parameters of the training loop \n",
    "total_interactions = 0 # total number of the interactions, that the agent had so far (each stack of the frames is counted once).\n",
    "\n",
    "\n",
    "# logging variables (accumulated over all episodes)\n",
    "history_of_total_losses = []\n",
    "history_of_total_rewards = []\n",
    "episode_cnt = 0\n",
    "num_of_last_episodes_to_avg = 100\n",
    "log_display_step = 10000\n",
    "start_time = time.time()\n",
    "\n",
    "print(f'Starting the Training...')\n",
    "while total_interactions < max_total_interactions: \n",
    "    episode_finished = False\n",
    "    episode_total_loss = 0.0\n",
    "    episode_total_reward = 0.0\n",
    "\n",
    "    # initializing a new episode\n",
    "    obs, info = wrapped_env.reset()\n",
    "    obs = torch.tensor(obs)\n",
    "\n",
    "    while not episode_finished:\n",
    "        # chosing action - observing the outcome - storing in replay buffer - learning \n",
    "        action = agent.choose_action(obs.unsqueeze(0).to(device))\n",
    "        next_obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "        next_obs, action = torch.tensor(next_obs), torch.tensor(action)\n",
    "        \n",
    "        agent.store_transition(obs, action, reward, terminated or truncated, next_obs)\n",
    "        loss = agent.learn()\n",
    "        \n",
    "        if loss == None: # it means that the replay buffer has not stored a sufficient number of transitions yet\n",
    "            continue\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        # logging (accumlated over each episode)\n",
    "        total_interactions += 1\n",
    "        episode_finished = terminated or truncated\n",
    "        episode_total_loss += loss\n",
    "        episode_total_reward += reward\n",
    "\n",
    "        # display logs every log_display_step + saving\n",
    "        if (total_interactions % log_display_step) == 0 and (total_interactions > 0) and (episode_cnt >= num_of_last_episodes_to_avg):\n",
    "            end_time = time.time()\n",
    "            avg_loss_of_last_episodes = np.average(history_of_total_losses[-num_of_last_episodes_to_avg:])\n",
    "            avg_reward_of_last_episodes = np.average(history_of_total_rewards[-num_of_last_episodes_to_avg:])\n",
    "            print(f'Displaying Logs at the Frame {total_interactions}, Episode {episode_cnt}, Delta Time: {end_time - start_time}')\n",
    "            print(f'Avg Loss Across {num_of_last_episodes_to_avg} Last Episodes = {avg_loss_of_last_episodes:.4f}')\n",
    "            print(f'Avg Reward Across {num_of_last_episodes_to_avg} Last Episodes = {avg_reward_of_last_episodes:.4f}')\n",
    "            start_time = end_time\n",
    "            \n",
    "            agent.save_model(f'saved_models/agent_it_{total_interactions}.pt')\n",
    "\n",
    "\n",
    "    # logging (accumulated over all episodes)\n",
    "    history_of_total_losses.append(episode_total_loss)\n",
    "    history_of_total_rewards.append(episode_total_reward)\n",
    "    episode_cnt += 1\n",
    "\n",
    "print(f'Training has been Finished!')\n",
    "\n",
    "print(f'Storing the Model...')\n",
    "dir_path = \"/kaggle/working/saved_models\"\n",
    "\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "    print(f\"Directory '{dir_path}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{dir_path}' already exists.\")\n",
    "agent.save_model(f'/kaggle/working/saved_models/agent_{game_id}.pt')\n",
    "\n",
    "print(f'Plotting the Logs...')\n",
    "plot_logs(game_id, total_interactions, episode_cnt, history_of_total_losses, history_of_total_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_for_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
