{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c076162f",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284b9bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# imports from memory.py\n",
    "import numpy as np\n",
    "\n",
    "# imports from wrappers.py\n",
    "import gymnasium as gym\n",
    "from PIL import Image\n",
    "from ale_py import ALEInterface\n",
    "\n",
    "# imports from agent.py \n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "# imports from train.py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Misc.\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597e4e9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d800b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNet(nn.Module):\n",
    "    def __init__(self, num_of_actions):\n",
    "        super(DQNNet, self).__init__()\n",
    "        self.num_of_actions = num_of_actions\n",
    "\n",
    "        # images should be preprocessed (extract luminance channel from RGB channels) by Ï† defined in the paper.\n",
    "\n",
    "        # in: (4, 84, 84) - out: (32, 20, 20)\n",
    "        # reLU should be applied on the outputs\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size= (8, 8), stride=4) \n",
    "\n",
    "        # in: (32, 20, 20) - out: (64, 9, 9)\n",
    "        # reLU should be applied on the outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size= (4, 4), stride=2)\n",
    "\n",
    "        # in: (64, 9, 9) - out: (64, 7, 7)\n",
    "        # reLU should be applied on the outputs\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size= (3, 3), stride=1)\n",
    "\n",
    "        # flattening should be applied here before feeding into fc1\n",
    "\n",
    "        # in: 64 * 7 * 7 = (3136, ) - out: (512, ) \n",
    "        # reLU should be applied on the outputs\n",
    "        self.fc1 = nn.Linear(in_features=3136, out_features=512)\n",
    "\n",
    "        # in: (512, ) - out: (7, )\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=num_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "         \n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "         \n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        x = nn.Flatten(start_dim=1, end_dim=-1)(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc74820",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d54fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_num_transitions, mini_batch_size, device=\"cpu\"):\n",
    "        self.lst = []\n",
    "        self.max_num_transitions = max_num_transitions\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.device = device\n",
    "\n",
    "    # this function returns the number of stored transitions\n",
    "    def __len__(self):\n",
    "        return len(self.lst)\n",
    "    \n",
    "    def append(self, curr_state, action, reward, next_state, done):\n",
    "        if len(self.lst) == self.max_num_transitions:\n",
    "            self.lst.pop(0)\n",
    "\n",
    "        trans = (curr_state, action, reward, next_state, done)\n",
    "        self.lst.append(trans)\n",
    "    \n",
    "    def sample_mini_batch(self):\n",
    "        if len(self.lst) < self.mini_batch_size:\n",
    "            raise Exception('Don\\'t try to sample mini-batches while number of stored transitions < mini_batch_size')\n",
    "        \n",
    "        idxs = np.random.randint(0, len(self.lst), self.mini_batch_size)\n",
    "        samples = [self.lst[idx] for idx in idxs]\n",
    "\n",
    "        zipped_content = tuple(zip(*samples))\n",
    "        dtypes = [torch.float32, torch.int, torch.bool, torch.float32, torch.float32]\n",
    "\n",
    "        mini_batch = [\n",
    "            torch.tensor(np.array(zipped_content[i]), dtype=dtypes[i]).to(self.device)\n",
    "            for i in range(len(zipped_content))\n",
    "        ]\n",
    "        \n",
    "        return mini_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6378f",
   "metadata": {},
   "source": [
    "# Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac47c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariImage(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Gym wrapper to preprocess the environments observations (frames)\n",
    "    The wrapper applies frameskip and stacks frames together\n",
    "    The same action is taken in each frame of a stack\n",
    "\n",
    "    :param env: Environment to wrap\n",
    "    :param image_shape: The output shape of the image\n",
    "    :param frame_skip: The amount of frames that stack, also the same action is applied\n",
    "    \"\"\"\n",
    "    def __init__(self, env, image_shape=(84, 84), frame_skip=4):\n",
    "        super().__init__(env)\n",
    "        self.image_shape = image_shape\n",
    "        self.frame_skip = frame_skip\n",
    "\n",
    "        obs_shape = (frame_skip, self.image_shape[0], self.image_shape[1])\n",
    "        self.observation_space = gym.spaces.Box(shape=obs_shape, low=0, high=1, dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        observations = []\n",
    "\n",
    "        raw_obs, info = self.env.reset()\n",
    "        obs = self._process_observations(raw_obs)\n",
    "        observations.append(obs)\n",
    "\n",
    "        for i in range(self.frame_skip - 1):\n",
    "            prev_raw_obs = raw_obs\n",
    "            raw_obs, reward, terminated, truncated, info = self.env.step(0) # Do nothing\n",
    "            obs = self._process_observations(raw_obs, prev_raw_obs)\n",
    "            observations.append(obs)\n",
    "\n",
    "        observation = np.stack(observations)\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        observations = []\n",
    "        total_reward = 0\n",
    "        prev_raw_obs = None\n",
    "        for i in range(self.frame_skip):\n",
    "            raw_obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "            obs = self._process_observations(raw_obs, prev_raw_obs)\n",
    "            observations.append(obs)\n",
    "            total_reward += reward\n",
    "            prev_raw_obs = raw_obs\n",
    "\n",
    "        observation = np.stack(observations)\n",
    "\n",
    "        return observation, total_reward, terminated, truncated, info\n",
    "\n",
    "    def _process_observations(self, raw_obs, prev_raw_obs = None):\n",
    "        if prev_raw_obs is not None: # if there is any previous observation\n",
    "            raw_obs = np.fmax(raw_obs, prev_raw_obs) # element-wise max between the two images, over all pixel colour values\n",
    "        image = Image.fromarray(raw_obs)\n",
    "        image = image.convert('L')\n",
    "        image = image.resize((self.image_shape[1], self.image_shape[0]))\n",
    "        image_array = np.array(image).astype(np.float32)\n",
    "        image_array /= 255\n",
    "        return image_array\n",
    "\n",
    "\n",
    "class ClipReward(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Gym wrapper to clip rewards\n",
    "\n",
    "    :param env: Environment to wrap\n",
    "    :param min_reward: The minimum reward\n",
    "    :param max_reward: The maximum reward\n",
    "    \"\"\"\n",
    "    def __init__(self, env, min_reward=-1, max_reward=1):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        reward = float(np.clip(reward, self.min_reward, self.max_reward))\n",
    "\n",
    "        return obs, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3877e2",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bfa8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_of_actions=7, network=None, lr=0.00025, gamma=0.99, eps=1.0,\n",
    "        eps_fframe=1e6, eps_final=0.1, minibatch_size=32, min_training_step=1000,\n",
    "        max_num_transitions=50000, target_interval=10000, device=\"cpu\"):   \n",
    "\n",
    "        self.num_of_actions = num_of_actions\n",
    "        if(network == None):\n",
    "            network = DQNNet(num_of_actions)\n",
    "        self.network = network.to(device)\n",
    "        self.target_network = copy.deepcopy(network)\n",
    "        self.target_interval = target_interval\n",
    "        self.learn_count = 0\n",
    "        # Hyperparameters taken from the paper\n",
    "        self.optim = torch.optim.RMSprop(network.parameters(), lr=lr, alpha=0.95, eps=0.01, momentum=0.95)\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "        self.eps = eps\n",
    "        self.eps_final = eps_final\n",
    "        self.eps_step = (eps - eps_final) / eps_fframe\n",
    "        self.gamma = gamma\n",
    "        self.min_training_step = min_training_step\n",
    "\n",
    "        self.memory = Memory(max_num_transitions=max_num_transitions, mini_batch_size=32, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.network.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        torch.save(self.network.state_dict(), model_path)\n",
    "\n",
    "    def store_transition(self, obs, action, reward, done, next_obs):\n",
    "        self.memory.append(obs, action, reward, done, next_obs)\n",
    "\n",
    "    def choose_action(self, obs, eps=None):\n",
    "        if(eps == None):\n",
    "            eps = self.eps\n",
    "\n",
    "        if(random.random() < eps):\n",
    "            return random.randint(0, self.num_of_actions - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action_values = self.network(obs)\n",
    "                return torch.argmax(action_values).item()\n",
    "\n",
    "    def learn(self):\n",
    "        if(len(self.memory) < self.min_training_step):\n",
    "            return\n",
    "\n",
    "        obss, actions, rewards, dones, next_obss = self.memory.sample_mini_batch()\n",
    "\n",
    "        ys = rewards + 0.0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            next_qvals = self.target_network(next_obss)\n",
    "            ys[dones == 0] += self.gamma * torch.max(next_qvals, dim=1)[0][dones == 0]\n",
    "        \n",
    "        qvals = self.network(obss)\n",
    "        ys_p = qvals[torch.arange(qvals.size(0), device=self.device), actions]\n",
    "\n",
    "        loss = F.mse_loss(ys, ys_p)\n",
    "\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        self.eps = max(self.eps - self.eps_step, self.eps_final)\n",
    "\n",
    "        self.learn_count += 1\n",
    "\n",
    "        if(self.learn_count % self.target_interval == 0):\n",
    "            self.target_network = copy.deepcopy(self.network)\n",
    "            print(\"Updated target network\")\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faeac4b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c79041db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, device, games_count=10):\n",
    "    scores = []\n",
    "    for i in range(games_count):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action_index = agent.choose_action(torch.tensor(obs).unsqueeze(0).to(device), eps=0.05)\n",
    "\n",
    "            obs, reward, done, truncated, info = env.step(action_index)\n",
    "            total_reward += reward\n",
    "\n",
    "        scores.append(total_reward)\n",
    "\n",
    "    mean_scores = sum(scores)/len(scores)\n",
    "\n",
    "    print(f\"Mean score: {mean_scores:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd5421",
   "metadata": {},
   "source": [
    "# Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e37e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logs(game_id, total_interactions, episode_cnt, history_of_total_losses, history_of_total_rewards):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    x = np.arange(1, episode_cnt + 1)\n",
    "    sns.lineplot(x=x, y=history_of_total_losses, ax=axs[0])\n",
    "    axs[0].set_title('Total Loss over Different Episodes')\n",
    "    axs[0].set_xlabel('Episodes')\n",
    "    axs[0].set_ylabel('Total MSE Loss')\n",
    "    # axs[0].legend()\n",
    "\n",
    "    sns.lineplot(x=x, y=history_of_total_rewards, ax=axs[1])\n",
    "    axs[1].set_title('Total Rewards over each Episodes')\n",
    "    axs[1].set_xlabel('Episodes')\n",
    "    axs[1].set_ylabel('Total Reward')\n",
    "    # axs[1].legend()\n",
    "\n",
    "    plt.suptitle('Total Loss & Reward over each Episodes \\n ' )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc1b0f",
   "metadata": {},
   "source": [
    "# Creating Directory for Saving the Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c21d7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'saved_models/' already exists.\n"
     ]
    }
   ],
   "source": [
    "# automatic directory creation for checkpoints\n",
    "dir_path = \"saved_models/\"\n",
    "\n",
    "if not os.path.exists(checkpoints_dir_path):\n",
    "    os.makedirs(checkpoints_dir_path)\n",
    "    print(f\"Directory '{checkpoints_dir_path}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{checkpoints_dir_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56dc3db",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32541668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Environment for the Game ALE/Breakout-v5 has been Initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m next_obs, action = torch.tensor(next_obs), torch.tensor(action)\n\u001b[32m     48\u001b[39m agent.store_transition(obs, action, reward, terminated \u001b[38;5;129;01mor\u001b[39;00m truncated, next_obs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m loss = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss == \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# it means that the replay buffer has not stored a sufficient number of transitions yet\u001b[39;00m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mAgent.learn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.optim.zero_grad()\n\u001b[32m     64\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mself\u001b[39m.eps = \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m.eps - \u001b[38;5;28mself\u001b[39m.eps_step, \u001b[38;5;28mself\u001b[39m.eps_final)\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m.learn_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_for_ai/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_for_ai/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_for_ai/lib/python3.12/site-packages/torch/optim/rmsprop.py:175\u001b[39m, in \u001b[36mRMSprop.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    163\u001b[39m     state_steps: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n\u001b[32m    165\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    166\u001b[39m         group,\n\u001b[32m    167\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    172\u001b[39m         state_steps,\n\u001b[32m    173\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[43mrmsprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43msquare_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43malpha\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmomentum\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcentered\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcentered\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_for_ai/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_for_ai/lib/python3.12/site-packages/torch/optim/rmsprop.py:511\u001b[39m, in \u001b[36mrmsprop\u001b[39m\u001b[34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, state_steps, foreach, maximize, differentiable, capturable, has_complex, lr, alpha, eps, weight_decay, momentum, centered)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    509\u001b[39m     func = _single_tensor_rmsprop\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43msquare_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcentered\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcentered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/env_for_ai/lib/python3.12/site-packages/torch/optim/rmsprop.py:406\u001b[39m, in \u001b[36m_multi_tensor_rmsprop\u001b[39m\u001b[34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, state_steps, lr, alpha, eps, weight_decay, momentum, centered, maximize, differentiable, capturable, has_complex)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.compiler.is_compiling() \u001b[38;5;129;01mand\u001b[39;00m grouped_state_steps[\u001b[32m0\u001b[39m].is_cpu:\n\u001b[32m    405\u001b[39m     torch._foreach_add_(\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         grouped_state_steps, \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, alpha=\u001b[32m1.0\u001b[39m\n\u001b[32m    407\u001b[39m     )\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    409\u001b[39m     torch._foreach_add_(grouped_state_steps, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# cofiguration of the environment\n",
    "game_id = 'ALE/Breakout-v5'\n",
    "max_total_interactions = 5000000\n",
    "frame_skip = 4\n",
    "env = gym.make(id=game_id, **{'frameskip':1})\n",
    "clip_reward_wrapper = ClipReward(env)\n",
    "atari_image_wrapper = AtariImage(clip_reward_wrapper)\n",
    "# add other wrappers if needed\n",
    "# ...\n",
    "wrapped_env = atari_image_wrapper # set to the last applied wrapper for more convinent naming \n",
    "\n",
    "print(f'The Environment for the Game {game_id} has been Initialized.')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# configuration of the agent\n",
    "agent = Agent( num_of_actions=4, device=device) # we keep the arguments as default\n",
    "\n",
    "\n",
    "# parameters of the training loop \n",
    "total_interactions = 0 # total number of the interactions, that the agent had so far (each stack of the frames is counted once).\n",
    "\n",
    "\n",
    "# logging variables (accumulated over all episodes)\n",
    "history_of_total_losses = []\n",
    "history_of_total_rewards = []\n",
    "episode_cnt = 0\n",
    "num_of_last_episodes_to_avg = 100\n",
    "log_display_step = 10000\n",
    "eval_cycle = 50000\n",
    "\n",
    "print(f'Starting the Training...')\n",
    "while total_interactions < max_total_interactions: \n",
    "    episode_finished = False\n",
    "    episode_total_loss = 0.0\n",
    "    episode_total_reward = 0.0\n",
    "\n",
    "    # initializing a new episode\n",
    "    obs, info = wrapped_env.reset()\n",
    "    obs = torch.tensor(obs)\n",
    "\n",
    "    while not episode_finished:\n",
    "        # chosing action - observing the outcome - storing in replay buffer - learning \n",
    "        action = agent.choose_action(obs.unsqueeze(0).to(device))\n",
    "        next_obs, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "        next_obs, action = torch.tensor(next_obs), torch.tensor(action)\n",
    "        \n",
    "        agent.store_transition(obs, action, reward, terminated or truncated, next_obs)\n",
    "        loss = agent.learn()\n",
    "        \n",
    "        if loss == None: # it means that the replay buffer has not stored a sufficient number of transitions yet\n",
    "            continue\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        # logging (accumlated over each episode)\n",
    "        total_interactions += 1\n",
    "        episode_finished = terminated or truncated\n",
    "        episode_total_loss += loss\n",
    "        episode_total_reward += reward\n",
    "\n",
    "        # display logs every log_display_step + saving\n",
    "        if (total_interactions % log_display_step) == 0 and (total_interactions > 0) and (episode_cnt >= num_of_last_episodes_to_avg):\n",
    "            avg_loss_of_last_episodes = np.average(history_of_total_losses[-num_of_last_episodes_to_avg:])\n",
    "            avg_reward_of_last_episodes = np.average(history_of_total_rewards[-num_of_last_episodes_to_avg:])\n",
    "            print(f'Displaying Logs at the Frame {total_interactions} and Episode {episode_cnt}:')\n",
    "            print(f'Avg Loss Across {num_of_last_episodes_to_avg} Last Episodes = {avg_loss_of_last_episodes:.4f}')\n",
    "            print(f'Avg Reward Across {num_of_last_episodes_to_avg} Last Episodes = {avg_reward_of_last_episodes:.4f}')\n",
    "\n",
    "            agent.save_model(f'saved_models/agent_it_{total_interactions}.pt')\n",
    "\n",
    "        if (total_interactions % eval_cycle and total_interactions > 0) == 0:\n",
    "            evaluate(wrapped_env, agent, device)\n",
    "\n",
    "    # logging (accumulated over all episodes)\n",
    "    history_of_total_losses.append(episode_total_loss)\n",
    "    history_of_total_rewards.append(episode_total_reward)\n",
    "    episode_cnt += 1\n",
    "\n",
    "print(f'Training has been Finished!')\n",
    "\n",
    "print(f'Storing the Model...')\n",
    "agent.save_model(f'{dir_path}agent_{game_id.replace('/', '_')}.pt')\n",
    "\n",
    "print(f'Plotting the Logs...')\n",
    "plot_logs(game_id, total_interactions, episode_cnt, history_of_total_losses, history_of_total_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_for_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
